{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc92573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728014d2",
   "metadata": {},
   "source": [
    "## Zipfiles Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b10ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3edcd783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words:  2007\n",
      "Negative words:  4784\n"
     ]
    }
   ],
   "source": [
    "master = \"MasterDictionary-20221105T045120Z-001.zip\"\n",
    "z = ZipFile(master)\n",
    "positive = str(z.read('MasterDictionary/positive-words.txt'))\n",
    "positive = positive.replace(\"\\\\n\",\" \")\n",
    "positive = positive.split()\n",
    "print(\"Positive words: \" ,len(positive))\n",
    "\n",
    "negative = str(z.read('MasterDictionary/negative-words.txt'))\n",
    "negative = negative.replace(\"\\\\n\",\" \")\n",
    "negative = negative.split()\n",
    "print(\"Negative words: \",len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16680daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words:  14236\n"
     ]
    }
   ],
   "source": [
    "stop_words = \"StopWords-20221105T045120Z-001.zip\"\n",
    "s = ZipFile(stop_words)\n",
    "s.infolist()\n",
    "\n",
    "stop01 = s.read('StopWords/StopWords_Generic.txt')\n",
    "stop02 = s.read('StopWords/StopWords_Currencies.txt')\n",
    "stop03 = s.read('StopWords/StopWords_Names.txt')\n",
    "stop04 = s.read('StopWords/StopWords_Geographic.txt')\n",
    "stop05 = s.read('StopWords/StopWords_GenericLong.txt')\n",
    "stop06 = s.read('StopWords/StopWords_Auditor.txt')\n",
    "stop07 = s.read('StopWords/StopWords_DatesandNumbers.txt')\n",
    "\n",
    "stop = stop01+stop02+stop03+stop04+stop05+stop06+stop07\n",
    "\n",
    "stop1 = stop.lower()\n",
    "stop1 = str(stop1)\n",
    "stop1 = stop1.replace(\"\\\\n\",\"\").replace(\"\\\\r\",\" \").replace(\"|\",\" \")\n",
    "stop1 = stop1.split()\n",
    "stop = stop1\n",
    "print(\"Stop words: \" ,len(stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e300d",
   "metadata": {},
   "source": [
    "## Url Text Extraction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b01b52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://insights.blackcoffer.com/what-jobs-will-robots-take-from-humans-in-the-future/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe1 = pd.read_excel('input.xlsx')\n",
    "dataframe1.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b382ff14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1799\n",
      "Word Count:  999\n",
      "Personal Pronouns:  |I: 0 |we: 8 |my: 0 |ours: 1 |us: 47 |\n",
      "Average Word Length :  7.530392156862745\n",
      "\n",
      "\n",
      "2 - POSITIVE SCORE:  46\n",
      "3 - NEGATIVE SCORE:  25\n",
      "4 - POLARITY SCORE:  0.2957746437214839\n",
      "5 - SUBJECTIVITY SCORE:  0.058101472947543795\n",
      "6 - AVERAGE SENTENCE LENGTH:  23.74025974025974\n",
      "7 - PERCENTAGE OF COMPLEX WORDS:  29.214402618657935\n",
      "8 - FOG INDEX :  21.18186494356707\n",
      "9 - AVG NUMBER OF WORDS PER SENTENCE :  13.246753246753247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',}\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "c = r.content\n",
    "\n",
    "soup = BeautifulSoup(c, \"html.parser\")\n",
    "title = soup.title.text.replace(\"- Blackcoffer Insights\", \"\")\n",
    "title = title.lower()\n",
    "title1 = title.split()\n",
    "\n",
    "para1 = soup.find(class_=\"td-post-content\").get_text().replace(\"\\n\",\" \").replace(\"\\xa0\", \"\")\n",
    "para2 = para1.replace(\".\",\"\").replace(',','').replace(\"(\",\" \").replace(\")\",\" \")\n",
    "para3 = para2.lower()\n",
    "para4 = para3.split()+title1\n",
    "print(len(para4))\n",
    "\n",
    "para_join = \" \".join(para4)\n",
    "para_join\n",
    "para = word_tokenize(para_join)\n",
    "\n",
    "para_t= para.copy()\n",
    "#print(\"Total length: \",len(para_t))\n",
    "i=0\n",
    "while i < len(para_t)-1:\n",
    "    if para_t[i] in stop:\n",
    "        para_t.pop(i)\n",
    "    i+=1\n",
    "paraf = para_t\n",
    "#print(\"Length after excluding stop words: \",len(paraf))\n",
    "\n",
    "words={}\n",
    "i=0\n",
    "while i < len(paraf)-1:\n",
    "    if paraf[i] in positive:\n",
    "        words[paraf[i]]=\"Positive\"\n",
    "    elif paraf[i] in negative:\n",
    "        words[paraf[i]]=\"Negative\"\n",
    "    i+=1\n",
    "#print(\"Positive and Negative words in Para: \", len(words))\n",
    "\n",
    "#1 - EXTRACTING DERIVED VARIABLES::\n",
    "#-------------------------------------------------------------------------------#\n",
    "p=0\n",
    "n=0\n",
    "for i in words:\n",
    "    if words[i]==\"Positive\":\n",
    "        p+=1\n",
    "    else:\n",
    "        n+=1\n",
    "#print(\"2 - Positive Score: \", p)\n",
    "#print(\"3 - Negative Score: \", n)\n",
    "\n",
    "#Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "#Ranges from -1 to +1\n",
    "\n",
    "pol = (p-n)/((p+n)+0.000001)\n",
    "#print(\"4 - Polarity Score: \", pol)\n",
    "\n",
    "#Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "#Ranges from 0 to 1\n",
    "\n",
    "sub = (p+n)/((len(paraf))+0.000001)\n",
    "#print(\"5 - Subjectivity Score: \", sub)\n",
    "#-------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#2 - ANALYSIS OF READABILITY\n",
    "#-------------------------------------------------------------------------------#\n",
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    counter = 0\n",
    "    is_previous_vowel = False\n",
    "    if word[len(word)-2]==\"e\" and word[len(word)-1]==\"d\":\n",
    "        word=word.replace(\"ed\",\"\")\n",
    "    elif word[len(word)-2]==\"e\" and word[len(word)-1]==\"s\":\n",
    "        word=word.replace(\"es\",\"\")\n",
    "    for index, value in enumerate(word):\n",
    "        if value in [\"a\", \"e\", \"i\", \"o\", \"u\", \"y\"]:\n",
    "            if index == len(word) - 1:\n",
    "                if value == \"e\":\n",
    "                    if counter == 0:\n",
    "                        counter += 1\n",
    "                else:\n",
    "                    counter += 1\n",
    "            else:\n",
    "                if is_previous_vowel == True:\n",
    "                    counter += 1\n",
    "                    is_previous_vowel = False\n",
    "                    break\n",
    "            is_previous_vowel = True\n",
    "        else:\n",
    "            if is_previous_vowel == True:\n",
    "                counter += 1\n",
    "            is_previous_vowel = False\n",
    "    return counter\n",
    "\n",
    "#-------------------------------------------------------------------------------#\n",
    "para_test = paraf.copy()\n",
    "i=0\n",
    "complex_words=[]\n",
    "syllables = []\n",
    "while i < len(para_test):\n",
    "    b=count_syllables(para_test[i])\n",
    "    syllables.append(b)\n",
    "    if b>2:\n",
    "        complex_words.append(para_test[i])\n",
    "    i+=1\n",
    "len(syllables)\n",
    "#-------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#4 - COMPLEX WORD COUNT::\n",
    "#-------------------------------------------------------------------------------#\n",
    "#print(\"Complex Word Count: \",len(complex_words))\n",
    "\n",
    "\n",
    "para_sent = sent_tokenize(para1)\n",
    "\n",
    "avg_sentence_length = len(para)/len(para_sent)\n",
    "#print(\"6 - Average Sentence Length: \", avg_sentence_length)\n",
    "\n",
    "complex_per = (len(complex_words)/len(para_test))*100\n",
    "#print(\"7 - Percentage of complex words: \", complex_per)\n",
    "\n",
    "fog_index = 0.4*(avg_sentence_length + complex_per)\n",
    "#print(\"8 - Fog Index : \",fog_index)\n",
    "#-------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#5 - WORD COUNT::\n",
    "#-------------------------------------------------------------------------------#\n",
    "text = para3\n",
    "text_tokens = word_tokenize(text)\n",
    "word_c = [word for word in text_tokens if not word in stopwords.words()]\n",
    "\n",
    "#print(\"Removed all the stop words using stopwords class of nltk: Length=> \", len(word_c))\n",
    "\n",
    "i=0\n",
    "word_count=[]\n",
    "while i < len(word_c):\n",
    "        if word_c[i]==\"?\":\n",
    "            pass\n",
    "        elif word_c[i]==\".\":\n",
    "            pass\n",
    "        elif word_c[i]==\",\":\n",
    "            pass\n",
    "        elif word_c[i]==\"!\":\n",
    "            pass\n",
    "        elif len(word_c[i])==1 and word_c[i] not in [\"a\",\"i\"]:\n",
    "            pass\n",
    "        else:\n",
    "            word_count.append(word_c[i])\n",
    "        i+=1\n",
    "    \n",
    "#print(\"After removing punctuations like (? ! , .) : Length => \", len(word_count))\n",
    "print(\"Word Count: \", len(word_count))\n",
    "#-------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#6 - SYLLABLE COUNT PER WORD ::\n",
    "#-------------------------------------------------------------------------------#\n",
    "\n",
    "syllables1 = dict(zip(para_test,syllables))\n",
    "#print(\"Syllable Count Per Word : \",\"\\n\", syllables1)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#7- COUNTING PESRSONAL PRONOUNS :: ( “I,” “we,” “my,” “ours,” and “us” )\n",
    "#-------------------------------------------------------------------------------#\n",
    "import re\n",
    "t = para3\n",
    "pattern = re.compile('I')\n",
    "matches = pattern.finditer(t)\n",
    "I_count=0\n",
    "for match in matches:\n",
    "    I_count+=1\n",
    "\n",
    "pattern = re.compile('we')\n",
    "matches = pattern.finditer(t)\n",
    "we_count=0\n",
    "for match in matches:\n",
    "    we_count+=1\n",
    "\n",
    "pattern = re.compile('my')\n",
    "matches = pattern.finditer(t)\n",
    "my_count=0\n",
    "for match in matches:\n",
    "    my_count+=1\n",
    "\n",
    "pattern = re.compile('ours')\n",
    "matches = pattern.finditer(t)\n",
    "ours_count=0\n",
    "for match in matches:\n",
    "    ours_count+=1\n",
    "\n",
    "pattern = re.compile('us')\n",
    "matches = pattern.finditer(t)\n",
    "us_count=0\n",
    "for match in matches:\n",
    "    us_count+=1\n",
    "#\n",
    "print(\"Personal Pronouns: \",\"|I:\",I_count,\"|we:\",we_count,\"|my:\",my_count,\"|ours:\", ours_count,\"|us:\",us_count,\"|\")\n",
    "#-------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#8 - AVERAGE WORD LENGTH::\n",
    "#-------------------------------------------------------------------------------#\n",
    "text_s = word_c\n",
    "a=0\n",
    "for i in text_s:\n",
    "    a=a+len(i)\n",
    "\n",
    "#print(\"Sum of all characters in each word : \", a)\n",
    "\n",
    "#print(\"Total number of words: \", len(word_c))\n",
    "\n",
    "avg_word_length = a/len(word_c)\n",
    "\n",
    "print(\"Average Word Length : \", avg_word_length)\n",
    "\n",
    "avg_words_per_sent = len(word_c)/len(para_sent)\n",
    "#print(\"9 - Average Words Per Sentence : \", avg_words_per_sent)\n",
    "#-------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"2 - POSITIVE SCORE: \", p)\n",
    "print(\"3 - NEGATIVE SCORE: \", n)\n",
    "print(\"4 - POLARITY SCORE: \", pol)\n",
    "print(\"5 - SUBJECTIVITY SCORE: \", sub)\n",
    "print(\"6 - AVERAGE SENTENCE LENGTH: \", avg_sentence_length)\n",
    "print(\"7 - PERCENTAGE OF COMPLEX WORDS: \", complex_per)\n",
    "print(\"8 - FOG INDEX : \",fog_index)\n",
    "print(\"9 - AVG NUMBER OF WORDS PER SENTENCE : \", avg_words_per_sent)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30c6b2",
   "metadata": {},
   "source": [
    "1.\tAll input variables in “Input.xlsx”\n",
    "2.\tPOSITIVE SCORE\n",
    "3.\tNEGATIVE SCORE\n",
    "4.\tPOLARITY SCORE\n",
    "5.\tSUBJECTIVITY SCORE\n",
    "6.\tAVG SENTENCE LENGTH\n",
    "7.\tPERCENTAGE OF COMPLEX WORDS\n",
    "8.\tFOG INDEX\n",
    "9.\tAVG NUMBER OF WORDS PER SENTENCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016ec7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     URL_ID                                                URL\n",
      "0        37  https://insights.blackcoffer.com/ai-in-healthc...\n",
      "1        38  https://insights.blackcoffer.com/what-if-the-c...\n",
      "2        39  https://insights.blackcoffer.com/what-jobs-wil...\n",
      "3        40  https://insights.blackcoffer.com/will-machine-...\n",
      "4        41  https://insights.blackcoffer.com/will-ai-repla...\n",
      "..      ...                                                ...\n",
      "109     146  https://insights.blackcoffer.com/blockchain-fo...\n",
      "110     147  https://insights.blackcoffer.com/the-future-of...\n",
      "111     148  https://insights.blackcoffer.com/big-data-anal...\n",
      "112     149  https://insights.blackcoffer.com/business-anal...\n",
      "113     150  https://insights.blackcoffer.com/challenges-an...\n",
      "\n",
      "[114 rows x 2 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd322a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201b360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
